{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweets_Analysis_using_Encoder_Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKKgMMm8x2yy3kMJBCeyIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9ac1dbfad434238a7df307f310b8f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce7b3314cd5248389e5ed6bb2d22d152",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_06798b012c864626935226c484035537",
              "IPY_MODEL_4ea204e62d0844b780962b12e44630e5"
            ]
          }
        },
        "ce7b3314cd5248389e5ed6bb2d22d152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06798b012c864626935226c484035537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_144eecb94b3a423bafdc7193865a0b42",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1364,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1364,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_444c555a9a6b4dac9a20f8ac00a624bc"
          }
        },
        "4ea204e62d0844b780962b12e44630e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_283484eecfec4451aac94040efed0870",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1364/1364 [00:00&lt;00:00, 4081.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2a85d9b90e141a29fb09d0d339034d8"
          }
        },
        "144eecb94b3a423bafdc7193865a0b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "444c555a9a6b4dac9a20f8ac00a624bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "283484eecfec4451aac94040efed0870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2a85d9b90e141a29fb09d0d339034d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajanm/END2_0_Session_6/blob/main/Tweets_Analysis_using_Encoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Rui41T1inL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940d16d3-ca8a-43a8-f3f2-9b7aebdde9bc"
      },
      "source": [
        "! pip install gdown==3.13.0 --quiet\n",
        "! pip install tweet-preprocessor --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0jml19D4EJD",
        "outputId": "7d72cdc4-4a01-4cd1-d194-58dcec96de6d"
      },
      "source": [
        "import gdown\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import preprocessor as tp\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "data_refresh = True\n",
        "data_url = 'https://drive.google.com/uc?id=1FWLOXtYDiOZckzvn79cZ7OeQCI9fVA00'\n",
        "data_file = 'tweets.csv'\n",
        "cleaned_data_file = 'tweets_cleaned.csv'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJZ-L8DK4xHh"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = tp.clean(text).strip()\n",
        "    text = text.replace(\":\", \"\")\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "d9ac1dbfad434238a7df307f310b8f16",
            "ce7b3314cd5248389e5ed6bb2d22d152",
            "06798b012c864626935226c484035537",
            "4ea204e62d0844b780962b12e44630e5",
            "144eecb94b3a423bafdc7193865a0b42",
            "444c555a9a6b4dac9a20f8ac00a624bc",
            "283484eecfec4451aac94040efed0870",
            "c2a85d9b90e141a29fb09d0d339034d8"
          ]
        },
        "id": "cToEO8oJ4LcS",
        "outputId": "5e4069cd-f9ce-4935-b02c-6391c6d4a012"
      },
      "source": [
        "%%time\n",
        "if data_refresh == True:\n",
        "  gdown.download(data_url, data_file)\n",
        "  dataset = pd.read_csv(data_file, header = 'infer')\n",
        "  dataset['clean_tweets'] = dataset['tweets'].progress_apply(clean_text)\n",
        "  dataset.to_csv(cleaned_data_file)\n",
        "else:\n",
        "  dataset = pd.read_csv(cleaned_data_file, header = 'infer')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FWLOXtYDiOZckzvn79cZ7OeQCI9fVA00\n",
            "To: /content/tweets.csv\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160k/160k [00:00<00:00, 27.0MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9ac1dbfad434238a7df307f310b8f16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1364.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CPU times: user 305 ms, sys: 25.2 ms, total: 331 ms\n",
            "Wall time: 1.25 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "zd8LRyW84-fR",
        "outputId": "f2cd634b-8104-4a47-b99d-486e175d0336"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1364.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.376833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.594859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            labels\n",
              "count  1364.000000\n",
              "mean      0.376833\n",
              "std       0.594859\n",
              "min       0.000000\n",
              "25%       0.000000\n",
              "50%       0.000000\n",
              "75%       1.000000\n",
              "max       2.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPYaMRvE7OkB",
        "outputId": "07c3b403-cff7-4335-a505-578101288d22"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1364 entries, 0 to 1363\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   tweets        1364 non-null   object\n",
            " 1   labels        1364 non-null   int64 \n",
            " 2   clean_tweets  1364 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 32.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNALwgpG81h5"
      },
      "source": [
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg0mtiN-9DjQ",
        "outputId": "f8e181b5-691e-4a0f-dff8-149ea97fae6c"
      },
      "source": [
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdd50dd44f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J33uyAH19GKp",
        "outputId": "846648bb-9080-4723-aa2d-8739dd769b62"
      },
      "source": [
        "%%time\n",
        "Tweet = torchtext.legacy.data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.3 s, sys: 263 ms, total: 1.56 s\n",
            "Wall time: 3.97 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je7N4g2k9Kex",
        "outputId": "9c27f9b0-5d5e-4aa7-eb03-8983fd112aa5"
      },
      "source": [
        "%%time\n",
        "Label = torchtext.legacy.data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 652 ms, sys: 75.7 ms, total: 728 ms\n",
            "Wall time: 724 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umSgI93l9VVI"
      },
      "source": [
        "fields = [('clean_tweets', Tweet), ('label', Label)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwgPeHbT9hRA",
        "outputId": "f411d7bd-a0f3-4f99-c5db-c3635728b859"
      },
      "source": [
        "%%time\n",
        "example = [torchtext.legacy.data.Example.fromlist([dataset.tweets[i],dataset.labels[i]], fields) for i in range(dataset.shape[0])]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 8s, sys: 1.26 s, total: 1min 9s\n",
            "Wall time: 1min 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeSy2tt9-IsC"
      },
      "source": [
        "example_dataset = torchtext.legacy.data.Dataset(example, fields)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7IT8BPt-fSo"
      },
      "source": [
        "(train, test) = example_dataset.split(split_ratio=[70, 30], random_state = random.seed(SEED))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjEi4WmQ-yrW",
        "outputId": "0cd5a723-4ca9-4109-b71b-7fef556d4963"
      },
      "source": [
        "print(len(train))\n",
        "print(len(test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "955\n",
            "409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHctmZkgCde_",
        "outputId": "86b3efd3-cb8d-49a4-e84b-67136e02fb0c"
      },
      "source": [
        "vars(train.examples[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clean_tweets': ['@TBCDG',\n",
              "  '#',\n",
              "  'WhatsRomneyHiding',\n",
              "  '-',\n",
              "  'OBAMA',\n",
              "  'wants',\n",
              "  'to',\n",
              "  'come',\n",
              "  'out',\n",
              "  'of',\n",
              "  'the',\n",
              "  'closet',\n",
              "  'but',\n",
              "  'Mitt',\n",
              "  'wants',\n",
              "  'to',\n",
              "  'wait',\n",
              "  'until',\n",
              "  'after',\n",
              "  'Elections',\n",
              "  '!',\n",
              "  '#',\n",
              "  'GOP',\n",
              "  '#',\n",
              "  'tcot'],\n",
              " 'label': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgdJC6a4_Ap4"
      },
      "source": [
        "Tweet.build_vocab(train)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgDuvTnh_IOg",
        "outputId": "45c941a8-1bf5-4244-846e-c09b724ba83a"
      },
      "source": [
        "print('Size of input vocab : ', len(Tweet.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  4148\n",
            "Size of label vocab :  3\n",
            "Top 10 words appreared repeatedly : [('Obama', 883), ('#', 644), (':', 644), ('.', 621), (',', 504), ('\"', 472), ('the', 458), ('RT', 411), ('?', 362), ('to', 340)]\n",
            "Labels :  defaultdict(None, {0: 0, 1: 1, 2: 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khfU-18S_KZA"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVu7JJ-2_OoY"
      },
      "source": [
        "train_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits((train, test), batch_size = 64, \n",
        "                                                            sort_key = lambda x: len(x.clean_tweets),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4bSIgVy_eZ5"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Tweet.vocab.stoi, tokens)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6I9RjMw_57I"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           batch_first=True)\n",
        "\n",
        "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, batch_first=True)\n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.embedding(text)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        packed_input = nn.utils.rnn.PackedSequence(torch.zeros(packed_output.data.shape[0],packed_output.data.shape[1]), packed_output.batch_sizes, packed_output.sorted_indices, packed_output.unsorted_indices)\n",
        "        packed_output, (hidden, cell) = self.decoder(packed_input.cuda(), (hidden,cell))\n",
        "    \n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zckfIZLAGD4"
      },
      "source": [
        "size_of_vocab = len(Tweet.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 3\n",
        "num_layers = 1\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2lQNnTARoP",
        "outputId": "4fc6739e-cbe8-4563-b504-1bbcc79924e7"
      },
      "source": [
        "%%time\n",
        "model=model.to(device)\n",
        "for batch in train_iterator:\n",
        "  tweet = batch.clean_tweets[0]\n",
        "  length = batch.clean_tweets[1]\n",
        "  model(tweet, length)\n",
        "  break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.1 s, sys: 864 ms, total: 12 s\n",
            "Wall time: 18.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNs8dx1JCxMv",
        "outputId": "584f0bf8-7a35-47bd-e65d-f00c6ce6cd82"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(4148, 300)\n",
            "  (encoder): LSTM(300, 100, batch_first=True)\n",
            "  (decoder): LSTM(100, 100, batch_first=True)\n",
            "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n",
            "The model has 1,486,303 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWQrAvfaC1Hf"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmHcxuYWC5ef"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.clean_tweets  \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.label)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzRFKkwHC8aH"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.clean_tweets\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_x_pnYxC-2f",
        "outputId": "52d6752b-ed2f-4e5d-f4fd-34405a8e7b10"
      },
      "source": [
        "%%time\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "model_path='./saved_weights.pt'\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    \n",
        "    print('\\n\\tEpoch No: ', epoch)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\tEpoch No:  0\n",
            "\tTrain Loss: 1.078 | Train Acc: 59.17%\n",
            "\t Val. Loss: 1.074 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  1\n",
            "\tTrain Loss: 1.067 | Train Acc: 69.79%\n",
            "\t Val. Loss: 1.063 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  2\n",
            "\tTrain Loss: 1.053 | Train Acc: 69.90%\n",
            "\t Val. Loss: 1.044 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  3\n",
            "\tTrain Loss: 1.020 | Train Acc: 69.90%\n",
            "\t Val. Loss: 0.991 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  4\n",
            "\tTrain Loss: 0.919 | Train Acc: 69.90%\n",
            "\t Val. Loss: 0.891 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  5\n",
            "\tTrain Loss: 0.863 | Train Acc: 69.90%\n",
            "\t Val. Loss: 0.883 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  6\n",
            "\tTrain Loss: 0.858 | Train Acc: 69.90%\n",
            "\t Val. Loss: 0.881 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  7\n",
            "\tTrain Loss: 0.855 | Train Acc: 69.90%\n",
            "\t Val. Loss: 0.881 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  8\n",
            "\tTrain Loss: 0.854 | Train Acc: 69.90%\n",
            "\t Val. Loss: 0.881 |  Val. Acc: 67.51% \n",
            "\n",
            "\n",
            "\tEpoch No:  9\n",
            "\tTrain Loss: 0.853 | Train Acc: 69.90%\n",
            "\t Val. Loss: 0.880 |  Val. Acc: 67.51% \n",
            "\n",
            "CPU times: user 2.81 s, sys: 141 ms, total: 2.95 s\n",
            "Wall time: 3.06 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5YvJ7doErBp"
      },
      "source": [
        "model.load_state_dict(torch.load(model_path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wYWG0pSaEzDO",
        "outputId": "db604b06-9ec0-4be5-9223-f80aa475de0b"
      },
      "source": [
        "classify_tweet(\"Obama will improve the policies for the people.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}